import streamlit as st
from bs4 import BeautifulSoup
import pandas as pd
import io
import re
from supabase import create_client, Client

# Ø¥Ø¹Ø¯Ø§Ø¯ Supabase
SUPABASE_URL = "https://ociaekhyqtiintzguudo.supabase.co"  # ØºÙŠÙ‘Ø± Ø¥Ù„Ù‰ Ø±Ø§Ø¨Ø· Ù…Ø´Ø±ÙˆØ¹Ùƒ
SUPABASE_KEY = "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6Im9jaWFla2h5cXRpaW50emd1dWRvIiwicm9sZSI6ImFub24iLCJpYXQiOjE3NjEzMjI0OTAsImV4cCI6MjA3Njg5ODQ5MH0.7yeAbnv2KUqaAvbyxr8mRvpG9oALl4k9mmJd3_UmwCU"  # ØºÙŠÙ‘Ø± Ø¥Ù„Ù‰ Ø§Ù„Ù€ anon key
BUCKET_NAME = "uploads"  # Ø§Ø³Ù… Ø§Ù„Ø¨ÙƒØª Ø¹Ù†Ø¯Ùƒ

supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)

st.title("uploads file")

uploaded_file = st.file_uploader("ğŸ“¤ Ø§Ø®ØªØ± Ù…Ù„Ù ASPX", type=["aspx"])

if uploaded_file is not None:
    try:
        # Ù‚Ø±Ø§Ø¡Ø© Ù…Ø­ØªÙˆÙ‰ Ù…Ù„Ù ASPX
        content = uploaded_file.read().decode("utf-8")
        soup = BeautifulSoup(content, "html.parser")
        tables = soup.find_all("table")

        all_rows = []
        current_semester, current_year = "", ""

        for table in tables:
            title_td = table.find("td", colspan=True)
            if title_td:
                title_text = title_td.get_text(strip=True)
                semester_match = re.search(r'(Ø§Ù„ÙØµÙ„\s+\S+)', title_text)
                year_match = re.search(r'(\d{4}/\d{4})', title_text)
                current_semester = semester_match.group(1) if semester_match else ""
                current_year = year_match.group(1) if year_match else ""
                continue

            for tr in table.find_all("tr"):
                if tr.find("td", colspan=True):
                    continue
                cells = [td.get_text(strip=True) for td in tr.find_all(["td", "th"])]
                if not cells:
                    continue
                all_rows.append([current_semester, current_year] + cells)

        if not all_rows:
            st.warning("âš ï¸ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø£ÙŠ Ø¨ÙŠØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ù…Ù„Ù.")
        else:
            # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„ ÙˆØ¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù Excel ÙÙŠ Ø§Ù„Ø°Ø§ÙƒØ±Ø©
            max_cols = max(len(r) for r in all_rows)
            for r in all_rows:
                while len(r) < max_cols:
                    r.append("")
            columns = ["Ø§Ù„ÙØµÙ„ Ø§Ù„Ø¯Ø±Ø§Ø³ÙŠ", "Ø§Ù„Ø³Ù†Ø© Ø§Ù„Ø¯Ø±Ø§Ø³ÙŠØ©"] + [f"Column{i}" for i in range(1, max_cols - 1)]
            df = pd.DataFrame(all_rows, columns=columns)

            # Ø­ÙØ¸ Ø§Ù„Ù…Ù„Ù ÙÙŠ Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø¨ØµÙŠØºØ© Excel
            excel_buffer = io.BytesIO()
            df.to_excel(excel_buffer, index=False)
            excel_buffer.seek(0)

            file_name = uploaded_file.name.replace(".aspx", ".xlsx")

            # Ø±ÙØ¹ Ø§Ù„Ù…Ù„Ù Ø¥Ù„Ù‰ Supabase Storage
            res = supabase.storage.from_(BUCKET_NAME).upload(
                file_name,
                excel_buffer.getvalue(),
                {"content-type": "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"}
            )

            if "error" in str(res).lower():
                st.error(f"âŒ Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø±ÙØ¹ Ø§Ù„Ù…Ù„Ù Ø¥Ù„Ù‰ Supabase: {res}")
            else:
                st.success(f"âœ… ØªÙ… ØªØ­ÙˆÙŠÙ„ ÙˆØ±ÙØ¹ Ø§Ù„Ù…Ù„Ù Ø¨Ù†Ø¬Ø§Ø­ Ø¥Ù„Ù‰ Supabase ({file_name})!")

    except Exception as e:
        st.error(f"âŒ Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©: {e}")
